<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How Neural Networks Work (With Simple Visuals)</title>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap" rel="stylesheet" />
  <style>
    body {
      font-family: 'Poppins', sans-serif;
      margin: 0;
      background-color: #f9f7ff;
      color: #333;
      line-height: 1.6;
    }

    header {
      background: linear-gradient(to right, #6a0dad, #ff6ec4);
      color: white;
      padding: 2rem;
      text-align: center;
    }

    header h1 {
      margin: 0;
      font-size: 2.5rem;
    }

    .content {
      padding: 2rem;
      max-width: 900px;
      margin: auto;
      background: white;
      box-shadow: 0 10px 20px rgba(106, 13, 173, 0.1);
      border-radius: 10px;
      margin-top: -2rem;
      z-index: 2;
      position: relative;
    }

    .content img {
  width: 50%;
  border-radius: 10px;
  margin: 1.5rem auto;
  display: block;
  box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
}

    .back-link {
      display: inline-block;
      margin-top: 2rem;
      padding: 0.5rem 1.5rem;
      background: linear-gradient(to right, #7b5aff, #ff6ec4);
      color: white;
      text-decoration: none;
      border-radius: 25px;
      font-weight: bold;
      transition: all 0.3s ease;
    }

    .back-link:hover {
      background: linear-gradient(to right, #ff6ec4, #7b5aff);
      transform: translateY(-2px);
    }

    h2 {
      color: #6a0dad;
      margin-top: 2rem;
      border-bottom: 2px solid #f0e6ff;
      padding-bottom: 0.5rem;
    }

    h3 {
      color: #8a4fff;
    }

    blockquote {
      background: #f9f5ff;
      border-left: 4px solid #8a4fff;
      padding: 1rem;
      margin: 1.5rem 0;
      font-style: italic;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
    }

    th, td {
      padding: 12px;
      text-align: left;
      border-bottom: 1px solid #e0d6ff;
    }

    th {
      background-color: #f3eeff;
      color: #6a0dad;
    }

    tr:hover {
      background-color: #faf7ff;
    }

    .math {
      background: #f5f2ff;
      padding: 1rem;
      border-radius: 5px;
      overflow-x: auto;
      font-family: monospace;
      margin: 1rem 0;
    }

    .caption {
      font-size: 0.9rem;
      color: #666;
      text-align: center;
      margin-top: -1rem;
      margin-bottom: 1.5rem;
    }

    .highlight {
      background-color: #f0e6ff;
      padding: 0.2rem 0.4rem;
      border-radius: 3px;
    }
  </style>
</head>
<body>
  <header>
    <h1>How Neural Networks Really Learn and Work</h1>
  </header>
  <div class="content">
    <img src="a1i1.webp" alt="Neural Network Diagram" />
    <p class="caption">Figure 1: A simple neural network with two hidden layers. Source: Wikipedia</p>

    <p>Artificial Neural Networks (ANNs) have become foundational to modern artificial intelligence, powering applications ranging from image recognition and language processing to autonomous vehicles. In this article, we explore how neural networks work—breaking down the key components, mechanisms, and learning processes—with simple visuals to make these complex systems accessible and engaging.</p>

    <h2>What Is a Neural Network?</h2>
    <p>A <span class="highlight">neural network</span> is a computational model inspired by the structure and functioning of the human brain. It consists of interconnected layers of <span class="highlight">artificial neurons</span>, or <em>nodes</em>, that work collaboratively to process information and learn patterns from data.</p>

    <h3>Key Layers in a Neural Network:</h3>
    <ul>
      <li><strong>Input Layer</strong>: Receives raw input data (e.g., pixel values from an image).</li>
      <li><strong>Hidden Layer(s)</strong>: Perform intermediate computations and extract patterns.</li>
      <li><strong>Output Layer</strong>: Produces the final result or prediction.</li>
    </ul>

    <h2>How a Single Neuron Works</h2>
    <p>An artificial neuron mimics the behavior of biological neurons. It accepts one or more inputs, computes a weighted sum, adds a bias, and applies an activation function to produce an output.</p>

    <div class="math">
      $$z = \sum_{i=1}^{n} w_i x_i + b,\quad y = g(z)$$
    </div>

    <p>Where:</p>
    <ul>
      <li>$x_i$: Input values</li>
      <li>$w_i$: Weights</li>
      <li>$b$: Bias</li>
      <li>$g$: Activation function (e.g., ReLU, Sigmoid)</li>
      <li>$y$: Neuron output</li>
    </ul>

    <img src="a1i2.jpeg" alt="Artificial Neuron" />
    <p class="caption">Figure 2: An artificial neuron model. Source: Medium</p>

    <h2>Forward Propagation</h2>
    <p>Forward propagation is the process by which data moves through the network:</p>
    <ol>
      <li>Inputs are passed into the network.</li>
      <li>Each neuron in a layer computes its output based on the inputs from the previous layer.</li>
      <li>The final output layer returns the network's prediction.</li>
    </ol>
    <p>This phase involves no learning; it's simply computation based on current parameters.</p>

    <h2>Training with Backpropagation</h2>
    <p>To make a neural network <em>learn</em>, we use a process called <span class="highlight">backpropagation</span> combined with <span class="highlight">gradient descent</span>:</p>
    <ol>
      <li><strong>Loss Function</strong>: Measures the difference between the predicted output and the actual label.</li>
      <li><strong>Gradient Computation</strong>: Calculates how much each weight contributes to the loss.</li>
      <li><strong>Weight Update</strong>: Weights are adjusted using gradient descent to minimize the loss over time.</li>
    </ol>

    <div class="math">
      $$w := w - \eta \cdot \frac{\partial L}{\partial w}$$
    </div>
    <p>Where $\eta$ is the learning rate, and $L$ is the loss.</p>

    <h2>Shallow vs. Deep Neural Networks</h2>
    <ul>
      <li><strong>Shallow Networks</strong>: Contain only one hidden layer.</li>
      <li><strong>Deep Neural Networks (DNNs)</strong>: Contain multiple hidden layers, enabling them to learn hierarchical and abstract representations.</li>
    </ul>
    <p>Deep networks are at the heart of <span class="highlight">deep learning</span>, which allows systems to outperform traditional models on complex tasks like image classification and natural language understanding.</p>

    <h2>Example: Handwritten Digit Recognition</h2>
    <p>Let's consider the MNIST dataset (28x28 pixel images of handwritten digits):</p>
    <ul>
      <li><strong>Input Layer</strong>: 784 neurons (28 × 28 pixels)</li>
      <li><strong>Hidden Layers</strong>: Extract edge, shape, and texture features</li>
      <li><strong>Output Layer</strong>: 10 neurons (representing digits 0 through 9)</li>
    </ul>
    <p>The network outputs probabilities for each digit class, and the highest probability determines the final prediction.</p>

    <h2>Why Neural Networks Are So Powerful</h2>
    <ul>
      <li><strong>Universal Function Approximators</strong>: Neural networks can approximate virtually any function with sufficient complexity.</li>
      <li><strong>Automatic Feature Learning</strong>: Unlike traditional models, they don't require manual feature engineering.</li>
      <li><strong>Versatile Applications</strong>: Used in vision, speech, language, robotics, and beyond.</li>
    </ul>

    <h2>Summary Table</h2>
    <table>
      <tr>
        <th>Component</th>
        <th>Description</th>
      </tr>
      <tr>
        <td>Neuron</td>
        <td>Performs a weighted sum + bias + activation</td>
      </tr>
      <tr>
        <td>Forward Propagation</td>
        <td>Passes input through the network to get predictions</td>
      </tr>
      <tr>
        <td>Backpropagation</td>
        <td>Adjusts weights using error feedback</td>
      </tr>
      <tr>
        <td>Activation Functions</td>
        <td>Introduce non-linearity (e.g., ReLU, Sigmoid)</td>
      </tr>
      <tr>
        <td>Deep Networks</td>
        <td>Enable learning of complex patterns across many layers</td>
      </tr>
    </table>

    <h2>Conclusion</h2>
    <p>Neural networks are powerful computational systems that emulate aspects of human cognition. By learning from data, they have become integral to modern AI—fueling advancements in computer vision, natural language processing, and decision-making systems. Understanding their structure and mechanics is crucial for anyone aspiring to work in AI.</p>

    <h3>References</h3>
    <ol>
      <li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li>
      <li>Nielsen, M. (2015). <em>Neural Networks and Deep Learning</em>.</li>
      <li>LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. <em>Nature</em>, 521(7553), 436–444.</li>
      <li>Chollet, F. (2017). <em>Deep Learning with Python</em>. Manning Publications.</li>
      <li>Wikipedia. <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial Neural Network</a></li>
    </ol>

    <a href="index.html" class="back-link">← Back to Portfolio</a>
  </div>
</body>
</html>